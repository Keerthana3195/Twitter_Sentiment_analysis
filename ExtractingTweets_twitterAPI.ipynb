{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5a1e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sending GET requests from the API\n",
    "import requests\n",
    "# For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os\n",
    "# For dealing with json responses we receive from the API\n",
    "import json\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "#To add wait time between requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f2b2a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the bearer token in the environment variable to use for authentication later\n",
    "# bearer token to be included in the empty quotes\n",
    "os.environ['TOKEN'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc64eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auth() that returns that bearer token stored in the environemnt \n",
    "def auth():\n",
    "    return os.getenv('TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b59ae42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will take our bearer token, pass it for authorization and return headers we will use to access the API\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a917df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we can access the API, we will build the request for the endpoint we are going to use and the params we want to pass\n",
    "def create_url(keyword, start_date, end_date, max_results = 500):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\" \n",
    "\n",
    "    # we can change parameters based on the requirement\n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad60b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84094934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(json_response, fileName):\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "        \n",
    "        # 2. Tweet text\n",
    "        tweet_text = tweet['text']\n",
    "\n",
    "        # 3. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 4. Geolocation\n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "\n",
    "        # 5. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 6. Language\n",
    "        lang = tweet['lang']\n",
    "\n",
    "        # 7. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "        \n",
    "        #8. retweeted_id and retweeted_type\n",
    "        if 'referenced_tweets' in tweet:\n",
    "            referenced_tweets_type = tweet['referenced_tweets'][0]['type']\n",
    "            retweeted_id = tweet['referenced_tweets'][0]['id']\n",
    "        else:\n",
    "            referenced_tweets_type = \" \"\n",
    "            retweeted_id = \" \"\n",
    "            \n",
    "            \n",
    "        # 9. source\n",
    "        source = tweet['source']\n",
    "\n",
    "        \n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [author_id,tweet_text,created_at,geo,tweet_id,lang,like_count,quote_count,reply_count,retweet_count,referenced_tweets_type,retweeted_id,source]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd96e87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2015-01-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  325\n",
      "Total # of Tweets added:  325\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2016-01-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  154\n",
      "Total # of Tweets added:  479\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2017-01-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  380\n",
      "Total # of Tweets added:  859\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  1jzu9lk96gu5npw44qf70phv54gfc74zb0m2qo17jw59\n",
      "Start Date:  2018-01-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  499\n",
      "Total # of Tweets added:  1358\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  1jzu9lk96gu5npw44qf70phv54gfc74zb0m2qo17jw59\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2018-01-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  11\n",
      "Total # of Tweets added:  1369\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fnlu0y9xx0vrdq1z19js2lzb90s4fx\n",
      "Start Date:  2019-01-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  483\n",
      "Total # of Tweets added:  1852\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  b26v89c19zqg8o3fnlu0y9xx0vrdq1z19js2lzb90s4fx\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2019-01-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  254\n",
      "Total # of Tweets added:  2106\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fosbuo5x3bteviuz9uzpxwgr1yn5a5\n",
      "Start Date:  2020-01-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  493\n",
      "Total # of Tweets added:  2599\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  b26v89c19zqg8o3fosbuo5x3bteviuz9uzpxwgr1yn5a5\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fo7ggh3yp87gdh888zli1kj117k5ml\n",
      "Start Date:  2020-01-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  480\n",
      "Total # of Tweets added:  3079\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpds6fpw6i0pyxar0rhd25vvrmfywt\n",
      "Start Date:  2021-01-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  486\n",
      "Total # of Tweets added:  3565\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  b26v89c19zqg8o3fpds6fpw6i0pyxar0rhd25vvrmfywt\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdg7ro4siu45kpawcpifzsu6f8ha5\n",
      "Start Date:  2021-01-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  498\n",
      "Total # of Tweets added:  4063\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpyzlf2mh7c07mhtp9ng6kc3dk8ku5\n",
      "Start Date:  2022-01-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  500\n",
      "Total # of Tweets added:  4563\n",
      "-------------------\n",
      "Total number of results:  4563\n"
     ]
    }
   ],
   "source": [
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "keyword = \"Vertiport lang:en\"\n",
    "start_list =    ['2015-01-01T00:00:00.000Z',\n",
    "                 '2016-01-01T00:00:00.000Z',\n",
    "                 '2017-01-01T00:00:00.000Z',\n",
    "                 '2018-01-01T00:00:00.000Z',\n",
    "                 '2019-01-01T00:00:00.000Z',\n",
    "                 '2020-01-01T00:00:00.000Z',\n",
    "                 '2021-01-01T00:00:00.000Z',\n",
    "                 '2022-01-01T00:00:00.000Z'\n",
    "                ]\n",
    "\n",
    "end_list =      ['2015-12-31T00:00:00.000Z',\n",
    "                 '2016-12-31T00:00:00.000Z',\n",
    "                 '2017-12-31T00:00:00.000Z',\n",
    "                 '2018-12-31T00:00:00.000Z',\n",
    "                 '2019-12-31T00:00:00.000Z',\n",
    "                 '2020-12-31T00:00:00.000Z',\n",
    "                 '2021-12-31T00:00:00.000Z',\n",
    "                 '2022-07-31T00:00:00.000Z'\n",
    "                 ]\n",
    "max_results = 500\n",
    "\n",
    "#Total number of tweets we collected from the loop\n",
    "total_tweets = 0\n",
    "\n",
    "# Create file\n",
    "csvFile = open(\"Vertiport.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "#Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "csvWriter.writerow(['author_id','tweet','created_at','geo','tweet_id','lang','like_count','quote_count','reply_count','retweet_count','referenced_tweets_type','retweeted_id','source'])\n",
    "csvFile.close()\n",
    "\n",
    "for i in range(0,len(start_list)):\n",
    "\n",
    "    # Inputs\n",
    "    count = 0 # Counting tweets per time period\n",
    "    max_count = 500 # Max tweets per time period\n",
    "    flag = True\n",
    "    next_token = None\n",
    "    \n",
    "    # Check if flag is true\n",
    "    while flag:\n",
    "        # Check if max_count reached\n",
    "        if count >= max_count:\n",
    "            break\n",
    "        print(\"-------------------\")\n",
    "        print(\"Token: \", next_token)\n",
    "        url = create_url(keyword, start_list[i],end_list[i], max_results)\n",
    "        json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "        result_count = json_response['meta']['result_count']\n",
    "\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            # Save the token to use for next call\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            print(\"Next Token: \", next_token)\n",
    "            if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                print(\"Start Date: \", start_list[i])\n",
    "                append_to_csv(json_response, \"Vertiport.csv\")\n",
    "                count += result_count\n",
    "                total_tweets += result_count\n",
    "                print(\"Total # of Tweets added: \", total_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(5)                \n",
    "        # If no next token exists\n",
    "        else:\n",
    "            if result_count is not None and result_count > 0:\n",
    "                print(\"-------------------\")\n",
    "                print(\"Start Date: \", start_list[i])\n",
    "                append_to_csv(json_response, \"Vertiport.csv\")\n",
    "                count += result_count\n",
    "                total_tweets += result_count\n",
    "                print(\"Total # of Tweets added: \", total_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(5)\n",
    "            \n",
    "            #Since this is the final request, turn flag to false to move to the next time period.\n",
    "            flag = False\n",
    "            next_token = None\n",
    "        time.sleep(5)\n",
    "print(\"Total number of results: \", total_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532dccdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
